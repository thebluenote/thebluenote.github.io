---
title: "Practical Machine Learning-Course Project"
output: html_document
---

### Introduction

With this project we are trying to predict the manner a physical exercise was performed using the data generated by sensors located on different parts of the body. The data and the original experiment description can be found here:  http://groupware.les.inf.puc-rio.br/har

### Data loading and exploratory data analysis

We first load the data and the required libraries:

```{r, message=FALSE, cache=FALSE, warning=FALSE}
setwd('~/Desktop/R_programming/04 ML/Project/')
test_set<-read.csv('pml-testing.csv')
training_set<-read.csv('pml-training.csv')
library(caret)
```

We will use the dataframe training\_set for model training and validation. The dataframe test\_set will be used to test the model.

We perform an exploratory data analysis to get to grips with the data and the problem:

```{r}
dim(test_set)
dim(training_set)
str(training_set$classe)
```

The training\_set is quite big (19622 observations of 160 variables), and the variable to be predicted (classe) is a factor variable with 5 levels: therefore, this will be a classification problem.

### Feature selection

The number of features is very big, so it will be wise to try to reduce it before running the classification algorithm. We first check if both the training and the test set have the same variables:

```{r}
setdiff(names(test_set), names(training_set))
setdiff(names(training_set), names(test_set))
```

So the variable classe is not present in the test\_set (as expected), and the variable 'problem\_id' is not present in the training\_set.

We now get rid of the bookkeeping variables not related to the experiment (variables one to seven)

```{r}
test_set<-test_set[,-seq(1,7)]
training_set<-training_set[,-seq(1,7)]
```

Now, we will check if there are NAs in test and training sets. We will use the following function:

```{r}
returnNA <- function (data){
  #Returns a vector with the count of NAs in columns with NAs
  columns.with.na<-colSums(is.na(data))
  columns.with.na[columns.with.na!=0]
}
```

which returns a vector with the number of NAs (if not 0) and whose names are the column names. Applying this function to our data:


```{r}
length(returnNA(training_set)) 
length(returnNA(test_set)) 
```
we observe that there are many columns with NAs in both sets, and since:
```{r}
setdiff(names(returnNA(training_set)), names(returnNA(test_set)))
```

we conclude that all of the columns with NAs in the training set (67) are also in the test set. 

Let's explore how many NAs are there in each set:

```{r}
table(returnNA(training_set))
table(returnNA(test_set))
```

In the test set, all these columns have all its values NAs, and in the training set, all of them have 19216 (out of 19622 observations) NAs (98%). Imputation of missing values does not seem then a good idea. Let's remove all of them:

```{r}
columnstoremove<-names(returnNA(training_set))
test_set<-test_set[,setdiff(colnames(test_set), columnstoremove)]
training_set<-training_set[,setdiff(colnames(training_set), columnstoremove)]
```

Finally, let's check if there are near zero variables:

```{r}
nzv<-nearZeroVar(training_set, saveMetrics = TRUE)
sum(nzv$nzv)
```

There are quite a few, so let's remove them:

```{r}
training_set<-training_set[, !nzv$nzv]
test_set<-test_set[, !nzv$nzv]
length(colnames(training_set))
```

We have therefore reduced the number of features from 160 to 53. This will greatly improve the accuracy of the classification algorithm and will also reduce training time.

### Data slicing

We split the training set in two sets in proportion 3/1. The biggest slice (training) will be used to train the model, and the smallest one will be used for validation to estimate the out-of-sample-error:

```{r}
set.seed(1234321)
inTrain <- createDataPartition(y=training_set$classe, p=3/4, list=F)
training<-training_set[inTrain, ]
validation<-training_set[-inTrain, ]
````

### Model fitting

We train a random forest with the training set using all the non-discarded variables as features:

```{r}
set.seed(666)
modFit<-train(classe~., method='rf', data=training)
```


### Model validation. Out-of-sample error


We now validate the model using the validation set:

```{r,message=FALSE, cache=FALSE, warning=FALSE}
predictions<- predict(modFit, newdata=validation)
confusionMatrix(testing$classe, predictions)
```

This gives us an accuracy of the classificator of 99.41%, with a 95% confidence interval of (99.15%, 99.60%). 

**Therefore, the estimate of the out-of-sample error is 0.59%,  with a 95% confidence interval of (0.40%, 0.85%)**

### Testing the model

This model classified correctly all instances of the test set (20/20)



